# 服务器代码分析

[xinnan-tech/xiaozhi-esp32-server: 本项目为xiaozhi-esp32提供后端服务，帮助您快速搭建ESP32设备控制服务器。Backend service for xiaozhi-esp32, helps you quickly build an ESP32 device control server.](https://github.com/xinnan-tech/xiaozhi-esp32-server)

使用版本Commit 29d59b3974d793367bedbd90afb332817370e3a2

## 文件分布

+ app.py 启动文件, 启动程序的入口, 使用aiohttp建立一个WebUI的服务器界面
+ + config文件夹, 配置文件的处理
    + + settings.py: 处理配置文件, 加载用户的配置项
      + private_config.py: 如果对不同的板子使用不同的配置, 在yaml里面设置use_private_config的时候会使用
      + logger.py: 日志文件
      + functionCallConfig.py: 默认使用的functionCall识别的函数以及参数
+ + core文件夹: 主要的逻辑代码
  + + handle文件夹: 和板子通信的接口以及处理意图识别和播放音乐的工具
      + providers文件夹: 进行语音识别, 使用的LLM, TTS供应商, 不同的组件需要实现base.py里面的通用接口, 如果想添加自己的接口, 可以参考这部分的内容
      + utils文件夹: 使用配置文件里面的配置加载providers里面的不同模型等部分, 还有同步使用的锁和处理p3格式音频使用的工具等
      + auth.py: 设备连接时候的验证
      + connection.py: 处理用户连接以后得交互, 以及和用户进行交互的主要流程
      + websocket_server.py: 处理用户的websocket连接
  + data: 配置文件和部分配置时候的聊天记忆可以放在这里
  + models: 使用的模型, 主要是语音识别和vad
  + music: 可以播放的音乐
  + tmp: 临时文件
+ config.yaml: 配置文件
+ performance_tester.py: 测试文件

## config文件夹

### setting.py

设置文件, 加载配置文件以及建立一个处理命令行输出的类, 可以使用`--config_path`参数设置实际的配置文件

还有一个更新配置文件的函数update_config

### private_config文件

私有的用户配置配置

## 主要的模块

### vad

**VAD**（Voice Activity Detection，[语音活动检测](https://so.csdn.net/so/search?q=语音活动检测&spm=1001.2101.3001.7020)）是一种技术，用于识别音频流中语音和非语音的区域。它能够区分语音和背景噪声，从而提高后续处理的效率和准确性。

主要使用这个模块进行检测音频的停止

### ASR

默认使用FanASR, 用于实现实时语音转录

[SenseVoice 实测，阿里开源语音大模型，识别效果和效率优于 Whisper，居然还能检测掌声、笑声！5分钟带你部署体验_sensevoicesmall开源-CSDN博客](https://blog.csdn.net/u010522887/article/details/140624599)

### 记忆_memory

+ mem_local_short

短期记忆, 使用yaml记录短期记忆, 在里面记录对话时候的主要信息, 比如用户名, 情感值等信息

如果想使用超长记忆，推荐使用mem0ai；如果注重隐私，请使用本地的mem_local_short

+ conn里面使用dialogue临时记录对话

### _tts

文字转语音模块

### _music

音乐处理, 管理本地可以播放的音乐, 不同格式的音频处理

### intent意图处理

所有的模型都可以使用, 但是实际的处理效果因为模型的大小和是否受过专门的训练而不同

## 主流程

+ 启动, 初始化服务器并等待连接

python app.py ==> 启动

检查一下配置文件以及ffmpeg的安装

加载一下配置文件

建立`WebSocketServer`, 使用配置文件里面的配置加载llm, tts之类的组件

+ 建立连接, 进行用户初始化以及验证

每次连接的时候使用_handle_connection函数处理链接, 处理的时候建立一个`ConnectionHandler`类处单个理用户的连接数据, 最终调用`handle_connection`函数进行连接处理

[通信协议：Websocket 连接 - 飞书云文档](https://ccnphfhqs21z.feishu.cn/wiki/M0XiwldO9iJwHikpXD5cEx71nKh)

在处理链接的时候首先获取用户的ip地址, 同时使用请求信息进行验证, 使用MAC地址作为机器的device_id(会使用这个作为记忆力的处理部分), 然后建立记忆力的处理和intent的处理句柄

如果配置了`use_private_config`, 会使用data/.private_config.yaml文件获取不同用户的配置

使用uuid生成一个随机的对话id, 使用yaml里面的xiaozhi配置发送一个hello信号

如果提示词里面有`{date_time}`, 使用当前时间进行替换

建立一个`tts_priority`线程用于处理tts任务, 尝试从tts_queue里面获取TTS的待处理音频

另一个是`audio_play_priority`线程, 用于处理播放音频的任务, 从`audio_play_queue`里面获取需要播放的任务, 发送给开发板, 这个里面的数据一般是在`_tts_priority_thread`任务里面放入的, 也有播放音乐的任务

最后使用`_route_message`循环处理`websocket`获取到的消息, 这里的消息可以有text或者bytes两种格式, 分别使用`handleTextMessage`和`handleAudioMessage`函数处理, text'可能是对话也可能是开发板的各种状态

`handleAudioMessage`获取到的数据都是对话的音频消息,所以可以使用, 首先把音频转换为文字, 之后进入对话处理

+ 处理对话, 对用户的对话进行不同的处理

最后处理对话文字的函数是`startToChat`

`startToChat`函数开始的时候首先进行意图检测, 如果可以检测出来用户的意图, 如果正常的聊天, 则不再进行后续的任务, 如果是播放音乐或者退出的意图, 进行使用function_call或者普通的chat进一步处理

实际的使用llm进行意图检测是在函数`analyze_intent_with_llm`里面进行, 在进行真正的检测之前, 使用`check_direct_exit`看是不是明确的退出命令, 是的话不进行下一步检测, 如果使用`use_function_call_mode`也不再进行下一步检测

**意识处理**

`analyze_intent_with_llm`实际进行意图检测, 使用`detect_intent`函数判断意图, 返回一个json字符串用于后续的意图判断, 如果大模型返回的不是json, 直接使用句子进行判断, 判断到是结束对话在发送一下stt消息以后使用`close_after_chat`对话之后结束通信, 音乐播放使用`handle_music_command`进行处理, 把识别到的音乐和本地的音乐进行匹配, 转换格式以后放入`audio_play_queue`里面

没有进行意识处理的任务会进入`send_stt_message`发送一个stt和一个llm消息

**function_call**

`chat_with_function_calling`函数是使用llm的函数调用的一种处理方式, 使用`get_functions`获取当前的所有的可以使用的函数, 实际记录在`FunctionCallConfig`里面, 使用插件的方式进行添加, 所有的插件试被func_handler管理

这个函数里面在初始的时候把这一次对话放入`dialogue`里面, 之后使用`query_memory`获取比较类似的对话记忆, 然后在函数`get_llm_dialogue_with_memory`里面使用合成新的system提示词, 在加上对话形成实际的llm对话内容在`response_with_functions`中获取模型的回答

在使用工具调用处理对话的时候使用`content`记录获取的参数放在`content_arguments`中进行后续函数调用的处理流程, 没有的时候使用用`response_message`记录对话, 获取的消息使用流式处理, 判断句子里面最后一个有效的标点依次处理, 标记句子的标号以后使用`speak_and_play`最后进tts, 发送给开发板

在实际进行function_call的时候首先使用`handle_llm_function_call`获取这次的function_call所调用的函数的结果, 然后使用_handle_function_result处理执行的结果, 在这个函数里面

1. RESPONSE, 把这个返回的字符串使用tts进行播放
2. REQLLM: 返回的结果需要再次使用LLM进行处理, 重新调用chat_with_function_calling函数, 传入function执行的返回值
3. 

## 实际使用

### 开发板

#### 文本通信接口

可以使用如下的接口使用文本消息进行通信

```python
{"type":"listen","state":"detect","text":"你好","source":"text"}
```

**Wake Word Detected**  

用于客户端向服务器告知检测到唤醒词。  例：

```json
{
  "session_id": "xxx",
  "type": "listen",
  "state": "detect",
  "text": "你好小明"
}
```

### 服务器

#### 文字转语音

```python
# 使用text_index对文字进行编号
await send_stt_message(conn, text)
future = self.executor.submit(self.speak_and_play, text, text_index)
self.tts_queue.put(future)
self.llm_finish_task = True
```

发送的语音会在_tts_priority_thread线程里面处理tts, 最后把待发送的音频放入audio_play_queue

#### 播放音乐

```python
play_local_music(self, conn, specific_file=None)
```

#### 工具调用

1. 在plugins_func文件夹里面的functions里面建立你自己的function文件夹, 使用下面的格式进行添加

```python
import requests
from bs4 import BeautifulSoup
from plugins_func.register import register_function,ToolType, ActionResponse, Action

# 描述一下这个function的实际功能好使用的参数
get_weather_function_desc = {
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "获取某个地点的天气，用户应先提供一个位置，比如用户说杭州天气，参数为：zhejiang/hangzhou，比如用户说北京天气怎么样，参数为：beijing/beijing。如果用户只问天气怎么样，参数是:guangdong/guangzhou",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {
                    "type": "string",
                    "description": "城市，zhejiang/hangzhou"
                }
            },
            "required": [
                "city"
            ]
        }
    }
}

@register_function('get_weather', get_weather_function_desc, ToolType.WAIT)
def get_weather(city: str):
    """
    "获取某个地点的天气，用户应先提供一个位置，\n比如用户说杭州天气，参数为：zhejiang/hangzhou，\n\n比如用户说北京天气怎么样，参数为：beijing/beijing",
    city : 城市，zhejiang/hangzhou
    """
    url = "https://tianqi.moji.com/weather/china/"+city
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
    }
    response = requests.get(url, headers=headers)
    if response.status_code!=200:
        return ActionResponse(Action.REQLLM, None, "请求失败")
    soup = BeautifulSoup(response.text, "html.parser")
    weather = soup.find('meta', attrs={'name':'description'})["content"]
    weather = weather.replace("墨迹天气", "")
    return ActionResponse(Action.REQLLM, weather, None) # 返回的ACTION会影响后面对于对话的影响
```

2. 实际的处理是在handle_llm_function_call函数里面, 根据不同的函数名字进行不同的处理, 目前需要根据你的FunctionCallConfig里面的配置自己在这个函数里面添加处理逻辑

#### 意图处理

IntentProviderBase里面添加你希望处理的intent, 之后在intent_llm里面是实际的处理位置, 可以在这里优化提示词`get_intent_system_prompt`

`process_intent_result`函数中处理最后的实际的意识部分结果, 在这里添加实际的处理方式

>  XvSenfeng 2025-3-14
